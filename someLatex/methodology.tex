
\section{Test Statistic}

In this section, we describe our new test procedure for hypotheses~\eqref{problem}. For simplicity, we now work on equal covariance setting and unequal covariance setting will be considered latter.
\begin{assumption}\label{theModel2}
Assume $V_1=V_2$, $D_1=D_2$, $\Lambda_1=\Lambda_2$, $\sigma_1=\sigma_2$ and $r_1=r_2$.
\end{assumption}

To simplify notations, the subscript $k$ of $\Sigma_k$, $V_k$, $D_k$, $\Lambda_k$, $\sigma_k$ and $r_k$ are dropped.
%\begin{equation}
%X_{ki}=\mu_k+V D U_{ki}+Z_{ki}.
%\end{equation}

In high dimensional setting, it is well-known that $S$ is singular when $p\geq n-1$.
As a result, Hotelling's $T^2$ statistic can not be defined.
Since $\Sigma$ has $p(p+1)/2$ independent parameters, it is hard to estimate when $p$ is large compared with $n$.
Therefore, for most recent work of high dimensional variance estimation, some additional assumptions, e.g.\ sparsity or low-rank, are adopted to regularize parameter space. For some recent development of this direction, see~\cite{Fan2015An}. 


Some existing tests for hypothesis~\eqref{problem} can be regarded as the likelihood ratio test (LRT) under restricted covariance matrix.
In fact, $\|\bar{X}_1-\bar{X}_2\|^2$, the main body of both $T_{BS}$ and $T_{CQ}$, is the LRT statistic assuming that $\Sigma=\sigma^2 I_p$ where $\sigma^2$ is unknown.
And $T_S$ is the LRT statistic assuming that $\Sigma$ is a diagonal matrix with unknown diagonal elements.
Although these methods are proved to be valid in more general setting, assumptions like~\eqref{chenscondition} are often adopted. 
In many applications, assumption~\eqref{chenscondition} may not be realistic due to the presence of common factors between variables.
We derive a test statistic suitable for such applications.

Consider the following restriction for $\Sigma$
\begin{equation}\label{andersonAssumption}
    \lambda_{r+1}(\Sigma)=\cdots=\lambda_p(\Sigma)=\sigma^2,
\end{equation}
    where $r$ is a known number and $\sigma^2>0$ is unknown.
    Under~\eqref{andersonAssumption},~\cite{Anderson1986Asymptotic} proved that the maximum likelihood estimator (MLE) of $\Sigma$ is $(n-2)n^{-1}\hat{\Sigma}$, where
    $$
    \hat{\Sigma}=\sum_{i=1}^r \hat{\lambda}_i \hat{u}_i \hat{u}_i^T
    +\hat{\sigma}^2 \hat{\tilde{V}}\hat{\tilde{V}}^T,
    $$
and $\hat{\sigma}^2=(p-r)^{-1}\sum_{i=r+1}^p \hat{\lambda}_i$.
    
Surprisingly, $\hat{\Sigma}$ is invertible even if $p\geq n-1$ and 
$$
\hat{\Sigma}^{-1}=\sum_{i=1}^r \hat{\lambda}_i^{-1} \hat{u}_i \hat{u}_i^T
    +\hat{\sigma}^{-2} \hat{\tilde{V}}\hat{\tilde{V}}^T.
$$
Thus, the LRT for hypothesis~\eqref{problem} always exists. And LRT reject the null hypothesis when
$$
T_{LRT}\overset{def}{=}\frac{1}{\tau}(\bar{X}_1-\bar{X}_2)^T \hat{\Sigma}^{-1}(\bar{X}_1-\bar{X}_2)
$$
is large.

The distribution of Hotelling's $T^2$ test statistic doesn't depend on unknown parameter.
Hence the critical value can be determined by exact distribution.
However, even if~\eqref{andersonAssumption} holds, the distribution of
$
T_{LRT}
$
still depends on parameters.
The critical value may be defined by asymptotic distribution or randomization methods.










